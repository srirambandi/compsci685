{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirambandi/compsci685/blob/main/train_and_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDNXFDkJfj2d",
        "outputId": "8af6469f-10f7-40c2-bbe8-d890ded78085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'compsci685'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 153 (delta 77), reused 103 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (153/153), 25.90 MiB | 3.93 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "/content/compsci685\n"
          ]
        }
      ],
      "source": [
        "# Colab Cell – clone your repo at the top of the notebook\n",
        "!git clone https://github.com/srirambandi/compsci685.git\n",
        "%cd compsci685"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Zcfni4zAYe",
        "outputId": "dfbcc69d-2fb3-491e-ee88-1958d408468e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets sympy tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfE1qm7Hf3dg"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"gen_dataset\")\n",
        "sys.path.insert(0, \"gen_dataset/src\")\n",
        "sys.path.insert(0, \"training/\") # treereg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wVrxNl94c69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from utils import prefix_to_sympy, verify_solution, OPERATORS\n",
        "\n",
        "from regularizer.regularizer_main import TreeRegularizer\n",
        "from parse_tree_adapted import get_parse_dict_for_prefix_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_C7ketupi8Q",
        "outputId": "3a4f951a-6da9-4c10-e859-2fa906e916fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/compsci685/checkpoints_treereg_paper_ode1_dataset\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYw2cfNuK4_O",
        "outputId": "0e83f4c1-f67a-4be2-bb99-ea02378040a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting Data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "683442it [00:00, 1510063.18it/s]\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"gen_dataset/data/train16_clean.txt\"\n",
        "# data is stored as input_equation\\toutput_equation\n",
        "\n",
        "TEST_SIZE = 1_024 # number of samples to evaluate model equation correctness on\n",
        "VALID_SIZE = 2_048 # number of samples to get validation loss from\n",
        "# hardcode these numbers, as 10% of new >600K sample dataset is too much\n",
        "# evaluation is slow, so only take 1024\n",
        "# validation is a bit faster, so take 2048\n",
        "# use those numbers as they are powers of 2 closest to 1k and 2k\n",
        "# so batch size will evenly divide them during validation\n",
        "\n",
        "os.makedirs(\"splits\", exist_ok=True)\n",
        "from tqdm import tqdm\n",
        "print(\"Splitting Data\")\n",
        "with open(DATA_PATH, 'r') as reader:\n",
        "  i = 0\n",
        "  with open(\"splits/test.txt\", 'w') as test_writer, open(\"splits/valid.txt\", 'w') as valid_writer, open(\"splits/train.txt\", 'w') as train_writer:\n",
        "    for line in tqdm(reader):\n",
        "      if i < TEST_SIZE:\n",
        "        test_writer.write(line + \"\\n\")\n",
        "      elif i < (TEST_SIZE + VALID_SIZE):\n",
        "        valid_writer.write(line + \"\\n\")\n",
        "      else:\n",
        "        train_writer.write(line + \"\\n\")\n",
        "\n",
        "      i += 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAf7IF5QK5G7",
        "outputId": "883753ae-c6e2-45ff-db4b-6264990b87f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1360740 splits/train.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l splits/train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywlJLwoyi2xw",
        "outputId": "2bfa43be-141b-4901-daf5-125f6722fa26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'E': 13, 'INT+': 14, 'INT-': 15, 'abs': 16, 'acos': 17, 'acosh': 18, 'add': 19, 'asin': 20, 'asinh': 21, 'atan': 22, 'atanh': 23, 'c': 24, 'cos': 25, 'cosh': 26, 'div': 27, 'exp': 28, 'log': 29, 'mul': 30, 'pi': 31, 'pow': 32, 'sign': 33, 'sin': 34, 'sinh': 35, 'sqrt': 36, 'tan': 37, 'tanh': 38, 'x': 39, 'y': 40, \"y'\": 41, '<pad>': 0, '<bos>': 1, '<eos>': 2}\n"
          ]
        }
      ],
      "source": [
        "# special tokens\n",
        "SPECIAL = {\"<pad>\":0,\"<bos>\":1,\"<eos>\":2}\n",
        "\n",
        "# collect tokens from original file:\n",
        "tokens = set()\n",
        "with open(DATA_PATH, 'r') as reader:\n",
        "  for line in reader:\n",
        "    tokens.update(line.split())\n",
        "word2idx = {w:i+len(SPECIAL) for i,w in enumerate(sorted(tokens))}\n",
        "word2idx.update(SPECIAL)\n",
        "print(word2idx)\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "PAD, BOS, EOS = word2idx[\"<pad>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]\n",
        "VOCAB_SIZE = len(word2idx)\n",
        "\n",
        "class ODEDataset(Dataset):\n",
        "    def __init__(self, data_file, max_len=18, word2idx_map=None, pad_idx=None, bos_idx=None, eos_idx=None):\n",
        "        self.src = []\n",
        "        self.tgt = []\n",
        "        with open(data_file, 'r') as reader:\n",
        "          for line in reader:\n",
        "            if \"\\t\" in line: # last line doesn't have any data\n",
        "              src_item, tgt_item = line.split(\"\\t\")\n",
        "              self.src.append(src_item.split())\n",
        "              self.tgt.append(tgt_item.split())\n",
        "        self.max_len = max_len\n",
        "        self.word2idx = word2idx_map\n",
        "        self.PAD_IDX = pad_idx\n",
        "        self.BOS_IDX = bos_idx\n",
        "        self.EOS_IDX = eos_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        src_str_tokens = self.src[i]\n",
        "        tgt_str_tokens = self.tgt[i]\n",
        "\n",
        "        src_ids = [self.BOS_IDX] + [self.word2idx.get(t, self.PAD_IDX) for t in src_str_tokens] + [self.EOS_IDX]\n",
        "        tgt_ids = [self.BOS_IDX] + [self.word2idx.get(t, self.PAD_IDX) for t in tgt_str_tokens] + [self.EOS_IDX]\n",
        "\n",
        "        # Pad sequences\n",
        "        padded_src_ids = src_ids[:self.max_len] + [self.PAD_IDX] * (self.max_len - len(src_ids))\n",
        "        if len(padded_src_ids) > self.max_len: padded_src_ids = padded_src_ids[:self.max_len] # Ensure fixed length\n",
        "\n",
        "        padded_tgt_ids = tgt_ids[:self.max_len] + [self.PAD_IDX] * (self.max_len - len(tgt_ids))\n",
        "        if len(padded_tgt_ids) > self.max_len: padded_tgt_ids = padded_tgt_ids[:self.max_len]\n",
        "\n",
        "\n",
        "        parse_d = get_parse_dict_for_prefix_list(src_str_tokens)\n",
        "        if parse_d is None:\n",
        "            parse_d = {}\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(padded_src_ids, dtype=torch.long),\n",
        "            \"src_len\": min(len(src_ids), self.max_len), # True length before padding (capped at max_len), including BOS/EOS\n",
        "            \"labels\": torch.tensor(padded_tgt_ids, dtype=torch.long),\n",
        "            \"tgt_len\": min(len(tgt_ids), self.max_len),\n",
        "            \"parses\": parse_d,\n",
        "            \"src_content_len\": len(src_str_tokens) # Number of actual words/tokens in source\n",
        "        }\n",
        "\n",
        "def collate_fn(batch_list):\n",
        "    input_ids_list = [item['input_ids'] for item in batch_list]\n",
        "    # Ensure all tensors in input_ids_list have the same length before stacking\n",
        "    # This should be guaranteed by __getitem__ if max_len is consistent\n",
        "    max_len_check = input_ids_list[0].size(0)\n",
        "    assert all(t.size(0) == max_len_check for t in input_ids_list), \"Padding error: Tensors in batch have different lengths.\"\n",
        "\n",
        "    input_ids = torch.stack(input_ids_list)\n",
        "    src_lengths = torch.tensor([item['src_len'] for item in batch_list], dtype=torch.long)\n",
        "\n",
        "    labels_list = [item['labels'] for item in batch_list]\n",
        "    assert all(t.size(0) == max_len_check for t in labels_list), \"Padding error: Label tensors have different lengths.\"\n",
        "    labels = torch.stack(labels_list)\n",
        "\n",
        "    tgt_lengths = torch.tensor([item['tgt_len'] for item in batch_list], dtype=torch.long)\n",
        "\n",
        "    parses_list = [item['parses'] for item in batch_list]\n",
        "    src_content_lengths = [item['src_content_len'] for item in batch_list]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"src_len\": src_lengths, # This is a tensor of varying lengths\n",
        "        \"labels\": labels,\n",
        "        \"tgt_len\": tgt_lengths, # This is a tensor of varying lengths\n",
        "        \"parses_batch\": parses_list,\n",
        "        \"src_content_lengths_batch\": src_content_lengths\n",
        "    }\n",
        "\n",
        "BATCH=256\n",
        "# Pass necessary mappings and indices to ODEDataset\n",
        "train_loader = DataLoader(ODEDataset(\"splits/train.txt\", word2idx_map=word2idx, pad_idx=PAD, bos_idx=BOS, eos_idx=EOS), batch_size=BATCH, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(ODEDataset(\"splits/valid.txt\", word2idx_map=word2idx, pad_idx=PAD, bos_idx=BOS, eos_idx=EOS), batch_size=BATCH, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(ODEDataset(\"splits/test.txt\", word2idx_map=word2idx, pad_idx=PAD, bos_idx=BOS, eos_idx=EOS), batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5NuiyJNjS3o"
      },
      "outputs": [],
      "source": [
        "# TODO: check if we should update this!! - a smaller model than the one in original Deep Learning for Symbolic Mathematcs paper\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=256,\n",
        "                 nhead=4,\n",
        "                 num_encoder_layers=4,\n",
        "                 num_decoder_layers=4,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.1,\n",
        "                 max_len=64,\n",
        "                 output_encoder_states=True):\n",
        "        super().__init__()\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(max_len, d_model))\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model, padding_idx=PAD)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model, nhead,\n",
        "            num_encoder_layers, num_decoder_layers,\n",
        "            dim_feedforward, dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.generator = nn.Linear(d_model, VOCAB_SIZE)\n",
        "\n",
        "        self.output_encoder_states = output_encoder_states\n",
        "        self.num_encoder_layers = num_encoder_layers\n",
        "\n",
        "    def encode(self, src, output_all_hidden_states_flag=False):\n",
        "        B, S = src.shape\n",
        "        src_emb = self.embedding(src) + self.pos_enc[:S].unsqueeze(0)\n",
        "        src_key_padding_mask = (src == PAD)\n",
        "\n",
        "        current_input = src_emb\n",
        "        all_hidden_states_list = []\n",
        "\n",
        "        for i in range(self.num_encoder_layers):\n",
        "            current_input = self.transformer.encoder.layers[i](\n",
        "                current_input,\n",
        "                src_key_padding_mask=src_key_padding_mask\n",
        "            )\n",
        "            if output_all_hidden_states_flag:\n",
        "                all_hidden_states_list.append(current_input)\n",
        "\n",
        "        memory = current_input\n",
        "\n",
        "        if output_all_hidden_states_flag:\n",
        "            return memory, all_hidden_states_list\n",
        "        return memory\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (B, S), tgt: (B, T)\n",
        "        B, S_len = src.shape\n",
        "        B_tgt, T_len = tgt.shape\n",
        "        assert B == B_tgt\n",
        "\n",
        "        src_emb = self.embedding(src) + self.pos_enc[:S_len].unsqueeze(0)\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_enc[:T_len].unsqueeze(0)\n",
        "\n",
        "        src_padding_mask = (src == PAD)\n",
        "        tgt_padding_mask = (tgt == PAD)\n",
        "        memory_padding_mask = src_padding_mask\n",
        "        tgt_causal_mask = self.transformer.generate_square_subsequent_mask(T_len).to(src.device)\n",
        "\n",
        "        all_encoder_hidden_states_output = None # Initialize\n",
        "\n",
        "        if self.output_encoder_states:\n",
        "            # Encode src and get all hidden states if needed\n",
        "            encoder_memory, all_encoder_hidden_states_output = self.encode(src, output_all_hidden_states_flag=True)\n",
        "\n",
        "            # Decode using the obtained encoder_memory\n",
        "            decoder_output = self.transformer.decoder(\n",
        "                tgt=tgt_emb,\n",
        "                memory=encoder_memory,\n",
        "                tgt_mask=tgt_causal_mask,\n",
        "                tgt_key_padding_mask=tgt_padding_mask,\n",
        "                memory_key_padding_mask=memory_padding_mask\n",
        "            )\n",
        "        else:\n",
        "            encoder_memory = self.encode(src, output_all_hidden_states_flag=False) # Just get final memory\n",
        "            decoder_output = self.transformer.decoder(\n",
        "                tgt=tgt_emb,\n",
        "                memory=encoder_memory,\n",
        "                tgt_mask=tgt_causal_mask,\n",
        "                tgt_key_padding_mask=tgt_padding_mask,\n",
        "                memory_key_padding_mask=memory_padding_mask\n",
        "            )\n",
        "\n",
        "        output_logits = self.generator(decoder_output)\n",
        "\n",
        "        if self.output_encoder_states:\n",
        "            return output_logits, all_encoder_hidden_states_output\n",
        "        return output_logits\n",
        "\n",
        "    def decode(self, tgt, memory):\n",
        "        B, T = tgt.shape\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_enc[:T].unsqueeze(0)\n",
        "        return self.transformer.decoder(\n",
        "            tgt_emb,\n",
        "            memory,\n",
        "            tgt_mask=self.transformer.generate_square_subsequent_mask(T).to(tgt.device),\n",
        "            tgt_key_padding_mask=tgt == PAD\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpqajPKj-Np"
      },
      "outputs": [],
      "source": [
        "# training and eval funcs go hereeeee\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2SeqTransformer().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "loss_fn   = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "\n",
        "tree_regularizer = TreeRegularizer(orth_bidir=True).to(device)\n",
        "treereg_alpha = 1  # 1, same as in paper\n",
        "global global_step_counter\n",
        "global_step_counter = 0\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss_epoch = 0\n",
        "    total_treereg_loss_epoch = 0\n",
        "    global global_step_counter\n",
        "    for batch_data in tqdm(train_loader):\n",
        "        global_step_counter += 1\n",
        "\n",
        "        src_tokens = batch_data[\"input_ids\"].to(device)\n",
        "        # slen_tensor = batch_data[\"src_len\"].to(device) # Tensor of actual lengths for each item\n",
        "        tgt_tokens = batch_data[\"labels\"].to(device)\n",
        "\n",
        "        parses_batch_from_loader = batch_data[\"parses_batch\"]\n",
        "        src_content_lengths_batch = batch_data[\"src_content_lengths_batch\"]\n",
        "\n",
        "        model_outputs = model(src_tokens, tgt_tokens[:,:-1]) # Decoder input excludes last token\n",
        "\n",
        "        all_encoder_hidden_states = None\n",
        "        if model.output_encoder_states:\n",
        "            output_logits, all_encoder_hidden_states = model_outputs\n",
        "        else:\n",
        "            output_logits = model_outputs\n",
        "\n",
        "        main_loss = loss_fn(output_logits.reshape(-1, VOCAB_SIZE), tgt_tokens[:,1:].reshape(-1)) # Loss against shifted target\n",
        "        current_batch_total_loss = main_loss\n",
        "\n",
        "        if model.output_encoder_states and (global_step_counter % 20 == 0) and all_encoder_hidden_states is not None:\n",
        "            if len(all_encoder_hidden_states) > 1: # Need at least 2 layers for index 1\n",
        "                hidden_states_for_treereg_all_batch = all_encoder_hidden_states[1] # 2nd layer\n",
        "\n",
        "                word_boundaries_for_treereg = []\n",
        "                parses_for_treereg_valid_items = []\n",
        "                valid_item_indices_in_batch = []\n",
        "\n",
        "                for i in range(src_tokens.size(0)):\n",
        "                    num_actual_tokens = src_content_lengths_batch[i]\n",
        "                    if num_actual_tokens > 0 and parses_batch_from_loader[i]: # Check parse dict is not empty\n",
        "                        word_boundaries_for_treereg.append([True] * num_actual_tokens)\n",
        "                        parses_for_treereg_valid_items.append(parses_batch_from_loader[i])\n",
        "                        valid_item_indices_in_batch.append(i)\n",
        "\n",
        "                if valid_item_indices_in_batch: # If any items are valid for TreeReg\n",
        "                    # Filter hidden states for valid items only\n",
        "                    # Ensure hidden_states_for_treereg_all_batch corresponds to src_tokens sequence length\n",
        "                    # The hidden states from encoder will have sequence length matching src_tokens (padded length)\n",
        "                    # However, SCINComputer processes based on actual token sequence length.\n",
        "                    # We need to handle this carefully. The `build_chart` in SCINComputer takes\n",
        "                    # hidden_states[idx].squeeze(0). If hidden_states is (B, Seq_padded, Dim),\n",
        "                    # then hidden_states[idx] is (Seq_padded, Dim).\n",
        "                    # The SCINComputer's internal indexing (st, en) for spans and word_boundaries\n",
        "                    # refers to the *actual* tokens, not padded.\n",
        "                    # So, hidden_states passed to build_chart should be (Num_valid_items, Max_actual_len_among_valid, Dim)\n",
        "                    # OR build_chart needs to be aware of padding for each item.\n",
        "                    # The current SCINComputer expects hidden_states[idx] to be for one sentence.\n",
        "                    # Let's pass the filtered batch of hidden states directly: (Num_valid_items, Seq_padded, Dim)\n",
        "                    # And `word_boundaries_for_treereg` has the actual lengths. SCINComputer should use those.\n",
        "\n",
        "                    filtered_hs_for_treereg = hidden_states_for_treereg_all_batch[torch.tensor(valid_item_indices_in_batch, device=device)]\n",
        "\n",
        "                    if filtered_hs_for_treereg.size(0) > 0:\n",
        "                        # The hidden states passed to build_chart are (N_valid, S_padded, D)\n",
        "                        # Word boundaries are List[List[bool]] with actual lengths\n",
        "                        # Parses are List[dict]\n",
        "                        charts = tree_regularizer.build_chart(filtered_hs_for_treereg, word_boundaries_for_treereg, None)\n",
        "\n",
        "                        try:\n",
        "                            reg_loss_terms, _ = tree_regularizer.get_score(charts, word_boundaries_for_treereg, parses_for_treereg_valid_items, device)\n",
        "                            valid_reg_losses = [l for l in reg_loss_terms if isinstance(l, torch.Tensor) and l.requires_grad]\n",
        "                            if valid_reg_losses:\n",
        "                                tree_reg_component_loss = torch.stack(valid_reg_losses).mean()\n",
        "                                current_batch_total_loss = current_batch_total_loss + (tree_reg_component_loss * treereg_alpha)\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error during TreeReg loss calculation (step {global_step_counter}): {e}\")\n",
        "                            # Potentially log more details: e.g., specific item causing error.\n",
        "                            print(f\"Problematic parses: {parses_for_treereg_valid_items}\")\n",
        "                            print(f\"Word boundaries: {word_boundaries_for_treereg}\")\n",
        "                            pass\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        current_batch_total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss_epoch += main_loss.item()\n",
        "        total_treereg_loss_epoch += current_batch_total_loss.item()\n",
        "\n",
        "    return total_loss_epoch / len(train_loader), total_treereg_loss_epoch / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch_data in loader:\n",
        "        src = batch_data[\"input_ids\"]\n",
        "        tgt = batch_data[\"labels\"]\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        out = model(src, tgt[:,:-1])\n",
        "        if model.output_encoder_states:\n",
        "            out, all_encoder_hidden_states = out\n",
        "        loss = loss_fn(out.reshape(-1, VOCAB_SIZE), tgt[:,1:].reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def greedy_decode(src, max_len=18):\n",
        "    src = src.to(device)\n",
        "    memory = model.encode(src)\n",
        "    ys = torch.full((src.size(0),1), BOS, device=device, dtype=torch.long)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(ys, memory)\n",
        "        prob = model.generator(out[:,-1,:])\n",
        "        next_word = prob.argmax(dim=-1, keepdim=True)\n",
        "        ys = torch.cat([ys, next_word], dim=1)\n",
        "        if (next_word==EOS).all(): break\n",
        "    return ys.cpu().tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONVjaxypkOvF",
        "outputId": "5f98104b-4bf5-4c33-f68f-ef579af79a87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2658 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 2658/2658 [01:52<00:00, 23.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train loss (CE) 1.3731 | val loss 0.8866 | CE loss + treereg loss 0.6564 | 112.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train loss (CE) 1.0222 | val loss 0.7720 | CE loss + treereg loss 0.0165 | 112.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train loss (CE) 0.9134 | val loss 0.7185 | CE loss + treereg loss -0.2019 | 112.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train loss (CE) 0.8494 | val loss 0.6806 | CE loss + treereg loss -0.3415 | 112.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | train loss (CE) 0.8058 | val loss 0.6488 | CE loss + treereg loss -0.4070 | 112.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | train loss (CE) 0.7731 | val loss 0.6376 | CE loss + treereg loss -0.4775 | 112.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | train loss (CE) 0.7466 | val loss 0.6218 | CE loss + treereg loss -0.5467 | 112.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | train loss (CE) 0.7294 | val loss 0.6022 | CE loss + treereg loss -0.6059 | 112.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | train loss (CE) 0.7149 | val loss 0.6136 | CE loss + treereg loss -0.6981 | 112.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:53<00:00, 23.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | train loss (CE) 0.7017 | val loss 0.5905 | CE loss + treereg loss -0.7541 | 113.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | train loss (CE) 0.6865 | val loss 0.5516 | CE loss + treereg loss -0.7889 | 112.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | train loss (CE) 0.6742 | val loss 0.5488 | CE loss + treereg loss -0.8891 | 112.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | train loss (CE) 0.6635 | val loss 0.5643 | CE loss + treereg loss -0.9129 | 112.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | train loss (CE) 0.6513 | val loss 0.5370 | CE loss + treereg loss -0.9652 | 112.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | train loss (CE) 0.6408 | val loss 0.5139 | CE loss + treereg loss -1.0318 | 112.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | train loss (CE) 0.6311 | val loss 0.5313 | CE loss + treereg loss -1.0342 | 112.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | train loss (CE) 0.6231 | val loss 0.5273 | CE loss + treereg loss -1.1085 | 112.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | train loss (CE) 0.6149 | val loss 0.5173 | CE loss + treereg loss -1.1388 | 112.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | train loss (CE) 0.6082 | val loss 0.5301 | CE loss + treereg loss -1.2377 | 113.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:53<00:00, 23.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 | train loss (CE) 0.6020 | val loss 0.5146 | CE loss + treereg loss -1.2511 | 113.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 | train loss (CE) 0.5960 | val loss 0.5246 | CE loss + treereg loss -1.2780 | 112.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 | train loss (CE) 0.5908 | val loss 0.5013 | CE loss + treereg loss -1.3833 | 111.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 | train loss (CE) 0.5853 | val loss 0.5144 | CE loss + treereg loss -1.4488 | 112.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:51<00:00, 23.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 | train loss (CE) 0.5806 | val loss 0.4827 | CE loss + treereg loss -1.4293 | 111.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 | train loss (CE) 0.5758 | val loss 0.4971 | CE loss + treereg loss -1.4893 | 112.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 | train loss (CE) 0.5701 | val loss 0.4877 | CE loss + treereg loss -1.5447 | 112.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:53<00:00, 23.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 | train loss (CE) 0.5652 | val loss 0.4933 | CE loss + treereg loss -1.6280 | 113.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 | train loss (CE) 0.5610 | val loss 0.4948 | CE loss + treereg loss -1.6669 | 112.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 | train loss (CE) 0.5566 | val loss 0.4793 | CE loss + treereg loss -1.7183 | 112.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:53<00:00, 23.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 | train loss (CE) 0.5523 | val loss 0.4673 | CE loss + treereg loss -1.7081 | 113.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31 | train loss (CE) 0.5490 | val loss 0.4609 | CE loss + treereg loss -1.7800 | 112.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32 | train loss (CE) 0.5451 | val loss 0.4685 | CE loss + treereg loss -1.8336 | 112.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33 | train loss (CE) 0.5405 | val loss 0.4654 | CE loss + treereg loss -1.9021 | 113.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34 | train loss (CE) 0.5378 | val loss 0.4764 | CE loss + treereg loss -1.8714 | 112.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35 | train loss (CE) 0.5339 | val loss 0.4577 | CE loss + treereg loss -1.9503 | 112.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36 | train loss (CE) 0.5307 | val loss 0.4569 | CE loss + treereg loss -1.9477 | 112.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37 | train loss (CE) 0.5263 | val loss 0.4554 | CE loss + treereg loss -2.0877 | 112.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38 | train loss (CE) 0.5229 | val loss 0.4509 | CE loss + treereg loss -2.1021 | 112.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39 | train loss (CE) 0.5198 | val loss 0.4759 | CE loss + treereg loss -2.1251 | 112.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:54<00:00, 23.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40 | train loss (CE) 0.5169 | val loss 0.4602 | CE loss + treereg loss -2.1833 | 114.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41 | train loss (CE) 0.5143 | val loss 0.4558 | CE loss + treereg loss -2.1842 | 112.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42 | train loss (CE) 0.5111 | val loss 0.4428 | CE loss + treereg loss -2.2412 | 112.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43 | train loss (CE) 0.5084 | val loss 0.4652 | CE loss + treereg loss -2.3072 | 112.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44 | train loss (CE) 0.5051 | val loss 0.4638 | CE loss + treereg loss -2.3079 | 112.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45 | train loss (CE) 0.5019 | val loss 0.4648 | CE loss + treereg loss -2.3717 | 112.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46 | train loss (CE) 0.4993 | val loss 0.4645 | CE loss + treereg loss -2.3922 | 112.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47 | train loss (CE) 0.4961 | val loss 0.4412 | CE loss + treereg loss -2.4962 | 112.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48 | train loss (CE) 0.4934 | val loss 0.4344 | CE loss + treereg loss -2.4438 | 112.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:53<00:00, 23.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 | train loss (CE) 0.4914 | val loss 0.4241 | CE loss + treereg loss -2.5630 | 113.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2658/2658 [01:52<00:00, 23.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 | train loss (CE) 0.4892 | val loss 0.4440 | CE loss + treereg loss -2.5803 | 112.3s\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "EPOCHS = 50\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # make faster\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss, with_treereg_loss = train_epoch()\n",
        "    val_loss   = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch} | train loss (CE) {train_loss:.4f} | val loss {val_loss:.4f} | CE loss + treereg loss {with_treereg_loss:.4f} | {time.time()-t0:.1f}s\")\n",
        "    torch.save(model.state_dict(), SAVE_DIR+f\"/epoch{epoch}.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfjkSAYrkU3I",
        "outputId": "3a8eb02f-886c-4b5e-a2c8-3eea2af09252"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [10:59<00:00, 20.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy semantic accuracy: 0.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#  testing here\n",
        "\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "x = sp.Symbol('x')\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "for batch_data in tqdm(test_loader):\n",
        "    src = batch_data[\"input_ids\"]\n",
        "    tgt = batch_data[\"labels\"]\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps = greedy_decode(src)       # list of B lists of token IDs - all hypostheses\n",
        "    truths = tgt.tolist()           # list of B lists - truths\n",
        "\n",
        "    for hyp_ids, true_ids in zip(hyps, truths):\n",
        "        # find first EOS and remove everything after it\n",
        "        try:\n",
        "          first_eos = hyp_ids.index(EOS)\n",
        "        except:\n",
        "          first_eos = len(hyp_ids) # if no EOS, don't strip\n",
        "\n",
        "        # strip special tokens\n",
        "        hyp_tok  = [idx2word[i] for i in hyp_ids[:first_eos]  if i not in (PAD, BOS, EOS)]\n",
        "        true_tok = [idx2word[i] for i in true_ids if i not in (PAD, BOS, EOS)]\n",
        "\n",
        "        # convert to Sympy and check\n",
        "        try:\n",
        "          hyp_expr = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "        except:\n",
        "          total += 1 # if model output isn't real equation, skip\n",
        "          continue\n",
        "        true_expr = prefix_to_sympy(true_tok, OPERATORS)\n",
        "        if verify_solution(hyp_expr, true_expr, rng):\n",
        "            n_correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "acc = 100 * n_correct / total\n",
        "print(f\"Greedy semantic accuracy: {acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QNgmj8xIod5n"
      },
      "outputs": [],
      "source": [
        "# write beam search here\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "\n",
        "BeamHyp = namedtuple(\"BeamHyp\", [\"score\", \"tokens\"])\n",
        "\n",
        "def beam_search(src_batch, beam_size=5, length_penalty=1.0, max_len=128):\n",
        "    \"\"\"\n",
        "    src_batch: LongTensor (B, S) - batch first as in the main model too\n",
        "    returns: list of B best token ID lists\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    B, S = src_batch.shape\n",
        "    src_batch = src_batch.to(device)\n",
        "    memory = model.encode(src_batch)\n",
        "\n",
        "    # initialize beams per example\n",
        "    beams = [[BeamHyp(0.0, [BOS])] for _ in range(B)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_beams = [[] for _ in range(B)]\n",
        "        for b in range(B):\n",
        "            for hyp in beams[b]:\n",
        "                tokens = hyp.tokens\n",
        "                # prepare decoder input: (1, t)\n",
        "                tgt_input = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "                dec = model.decode(tgt_input, memory[b:b+1])     # (1, t, D)\n",
        "                # project last step to vocab & log‐softmax\n",
        "                logits = model.generator(dec[:, -1, :])          # (1, V)\n",
        "                logp   = F.log_softmax(logits, dim=-1).squeeze(0) # (V,)\n",
        "\n",
        "                topv, topi = logp.topk(beam_size)\n",
        "                for score, idx in zip(topv.tolist(), topi.tolist()):\n",
        "                    all_beams[b].append(BeamHyp(hyp.score + score, tokens + [idx]))\n",
        "\n",
        "            # prune back to beam_size\n",
        "            all_beams[b].sort(\n",
        "                key=lambda h: h.score / (len(h.tokens) ** length_penalty),\n",
        "                reverse=True\n",
        "            )\n",
        "            beams[b] = all_beams[b][:beam_size]\n",
        "\n",
        "    # extract all beams, and count as correct if at least one beam from hypothesis is correct\n",
        "    results = []\n",
        "    for b in range(B):\n",
        "        b_beams = beams[b]\n",
        "        out_beams = [a.tokens for a in b_beams]\n",
        "        results.append(out_beams)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq3sh8hDDpPv",
        "outputId": "770ad136-e067-4a2a-cea6-4c6f5a5b7dca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [2:21:53<00:00, 266.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam-10 semantic accuracy: 1.05%\n",
            "model ['mul', 'c', 'pow', 'add', 'INT+', '6', 'mul', 'INT+', '2', 'x', 'INT-', '1']\n",
            "gt ['mul', 'c', 'pow', 'add', 'INT+', '4', 'mul', 'INT+', '2', 'x', 'INT-', '1']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# now evaluate with beam=10\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "for batch_data in tqdm(test_loader):\n",
        "    src, tgt = batch_data[\"input_ids\"], batch_data[\"labels\"]\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps   = beam_search(src, beam_size=10, length_penalty=1.0, max_len=18)\n",
        "    truths = tgt.tolist()\n",
        "\n",
        "    for beam, true_ids in zip(hyps, truths):\n",
        "        for hyp_ids in beam: # if one item from beams is correct, then count sample as correct\n",
        "          # find first EOS and remove everything after it\n",
        "          try:\n",
        "            first_eos = hyp_ids.index(EOS)\n",
        "          except:\n",
        "            first_eos = len(hyp_ids) # if no EOS, don't strip\n",
        "\n",
        "          # strip special tokens\n",
        "          hyp_tok  = [idx2word[i] for i in hyp_ids[:first_eos]  if i not in (PAD, BOS, EOS)]\n",
        "          true_tok = [idx2word[i] for i in true_ids if i not in (PAD, BOS, EOS)]\n",
        "\n",
        "          # convert to Sympy and check\n",
        "          try:\n",
        "            hyp_expr = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "          except:\n",
        "            total += 1\n",
        "            continue # if model output isn't real equation, skip\n",
        "          true_expr = prefix_to_sympy(true_tok, OPERATORS)\n",
        "          if verify_solution(hyp_expr, true_expr, rng):\n",
        "              n_correct += 1\n",
        "              break # this input's set of beams has a correct answer, skip the rest (any correct = correct)\n",
        "        total += 1\n",
        "\n",
        "\n",
        "acc = 100 * n_correct / total\n",
        "print(f\"Beam-10 semantic accuracy: {acc:.2f}%\")\n",
        "print(\"model\", hyp_tok)\n",
        "print(\"gt\", true_tok)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
