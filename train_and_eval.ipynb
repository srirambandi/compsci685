{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZF2a77tfJ6Y1cWnEDAngcPcqd345kJRv",
      "authorship_tag": "ABX9TyMJsgHNc79qRFyyXgWBuhCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirambandi/compsci685/blob/main/train_and_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Cell – clone your repo at the top of the notebook\n",
        "!git clone https://github.com/srirambandi/compsci685.git\n",
        "%cd compsci685"
      ],
      "metadata": {
        "id": "MDNXFDkJfj2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Zcfni4zAYe"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets sympy tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"gen_dataset\")\n",
        "sys.path.insert(0, \"gen_dataset/src\")"
      ],
      "metadata": {
        "id": "VfE1qm7Hf3dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from utils import prefix_to_sympy, verify_solution, OPERATORS"
      ],
      "metadata": {
        "id": "6wVrxNl94c69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/compsci685/checkpoints\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "8_C7ketupi8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"gen_dataset/data/fin_dataset.csv\"\n",
        "cols = [\"id\", \"equ_str\", \"equ_prefix\", \"sol_str\", \"sol_prefix\"]\n",
        "\n",
        "df = pd.read_csv(DATA_PATH, header=0, names=cols)\n",
        "print(f\"Total examples: {len(df)}\")\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "gW5GaQqd48vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stats about our dataset\n",
        "df[\"equ_len\"] = df[\"equ_prefix\"].str.split().apply(len)\n",
        "df[\"sol_len\"] = df[\"sol_prefix\"].str.split().apply(len)\n",
        "\n",
        "df[[\"equ_len\", \"sol_len\"]].describe().T"
      ],
      "metadata": {
        "id": "Ut7i4giu5E0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's split the data into train, test and val like in the paper: https://github.com/facebookresearch/SymbolicMathematics/blob/main/split_data.py\n",
        "\n",
        "N = len(df)\n",
        "m = int(0.1 * N)       # let's take 10% as validation data set size for now.\n",
        "assert 2 * m < N, \"Pick smaller m!\"\n",
        "\n",
        "alpha = math.log(N - 0.5) / math.log(2 * m)\n",
        "\n",
        "raw_idxs = [int(i**alpha) for i in range(1, 2*m + 1)]\n",
        "val_idxs  = set(raw_idxs[::2])\n",
        "test_idxs = set(raw_idxs[1::2])\n",
        "\n",
        "all_idxs   = set(range(N))\n",
        "train_idxs = all_idxs - val_idxs - test_idxs\n",
        "\n",
        "# slice and reset index\n",
        "train_df = df.iloc[sorted(train_idxs)].reset_index(drop=True)\n",
        "val_df   = df.iloc[sorted(val_idxs)].reset_index(drop=True)\n",
        "test_df  = df.iloc[sorted(test_idxs)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Split sizes => train: {len(train_df)}, valid: {len(val_df)}, test: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "CJA8E8pkFA2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"splits\", exist_ok=True)\n",
        "\n",
        "train_df.to_csv(\"splits/train.csv\", index=False)\n",
        "val_df.to_csv(\"splits/valid.csv\", index=False)\n",
        "test_df.to_csv(\"splits/test.csv\", index=False)\n",
        "\n",
        "print(\"Saved splits into ./splits/\")"
      ],
      "metadata": {
        "id": "Wmi3UiMmF7Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# special tokens\n",
        "SPECIAL = {\"<pad>\":0,\"<bos>\":1,\"<eos>\":2}\n",
        "\n",
        "# collect tokens from training\n",
        "tokens = {t for seq in train_df.equ_prefix.str.split() for t in seq}\n",
        "tokens |= {t for seq in train_df.sol_prefix.str.split() for t in seq}\n",
        "word2idx = {w:i+len(SPECIAL) for i,w in enumerate(sorted(tokens))}\n",
        "word2idx.update(SPECIAL)\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "PAD, BOS, EOS = word2idx[\"<pad>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]\n",
        "VOCAB_SIZE = len(word2idx)\n",
        "\n",
        "class ODEDataset(Dataset):\n",
        "    def __init__(self, df, src_col, tgt_col, max_len=128):\n",
        "        self.src = df[src_col].str.split().tolist()\n",
        "        self.tgt = df[tgt_col].str.split().tolist()\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "    def __getitem__(self, i):\n",
        "        src = [BOS] + [word2idx[t] for t in self.src[i]] + [EOS]\n",
        "        tgt = [BOS] + [word2idx[t] for t in self.tgt[i]] + [EOS]\n",
        "        def pad(x):\n",
        "            x = x[:self.max_len]\n",
        "            return x + [PAD]*(self.max_len-len(x))\n",
        "        return torch.tensor(pad(src)), len(src), torch.tensor(pad(tgt)), len(tgt)\n",
        "\n",
        "def collate(batch):\n",
        "    srcs, slens, tgts, tlens = zip(*batch)\n",
        "    return (torch.stack(srcs), torch.tensor(slens)), (torch.stack(tgts), torch.tensor(tlens))\n",
        "\n",
        "BATCH=32\n",
        "train_loader = DataLoader(ODEDataset(train_df,\"equ_prefix\",\"sol_prefix\"), batch_size=BATCH, shuffle=True, collate_fn=collate)\n",
        "val_loader   = DataLoader(ODEDataset(val_df,  \"equ_prefix\",\"sol_prefix\"), batch_size=BATCH, shuffle=False, collate_fn=collate)\n",
        "test_loader  = DataLoader(ODEDataset(test_df, \"equ_prefix\",\"sol_prefix\"), batch_size=BATCH, shuffle=False, collate_fn=collate)\n"
      ],
      "metadata": {
        "id": "ywlJLwoyi2xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: check if we should update this!! - a smaller model than the one in original Deep Learning for Symbolic Mathematcs paper\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=256,\n",
        "                 nhead=4,\n",
        "                 num_encoder_layers=4,\n",
        "                 num_decoder_layers=4,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.1,\n",
        "                 max_len=512):\n",
        "        super().__init__()\n",
        "        # positional embeddings: (max_len, d_model)\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(max_len, d_model))\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model, padding_idx=PAD)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model, nhead,\n",
        "            num_encoder_layers, num_decoder_layers,\n",
        "            dim_feedforward, dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.generator = nn.Linear(d_model, VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (S, B), tgt: (T, B)\n",
        "        B, S = src.shape\n",
        "        B2, T = tgt.shape\n",
        "        assert B == B2\n",
        "\n",
        "        # (B,S,d_model) + (1,S,d_model) => broadcast to (B,S,d)\n",
        "        src_emb = self.embedding(src) + self.pos_enc[:S].unsqueeze(0)\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_enc[:T].unsqueeze(0)\n",
        "\n",
        "\n",
        "        out = self.transformer(\n",
        "            src_emb, tgt_emb,\n",
        "            src_key_padding_mask=src == PAD,\n",
        "            tgt_key_padding_mask=tgt == PAD,\n",
        "            memory_key_padding_mask=src == PAD,\n",
        "            tgt_mask=self.transformer.generate_square_subsequent_mask(T).to(src.device)\n",
        "        )\n",
        "        return self.generator(out)  # (B, T, V) expected here\n",
        "\n",
        "    def encode(self, src):\n",
        "        B, S = src.shape\n",
        "        src_emb = self.embedding(src) + self.pos_enc[:S].unsqueeze(0)\n",
        "        return self.transformer.encoder(\n",
        "            src_emb,\n",
        "            src_key_padding_mask=src == PAD\n",
        "        )\n",
        "\n",
        "    def decode(self, tgt, memory):\n",
        "        B, T = tgt.shape\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_enc[:T].unsqueeze(0)\n",
        "        return self.transformer.decoder(\n",
        "            tgt_emb,\n",
        "            memory,\n",
        "            tgt_mask=self.transformer.generate_square_subsequent_mask(T).to(tgt.device),\n",
        "            tgt_key_padding_mask=tgt == PAD\n",
        "        )\n"
      ],
      "metadata": {
        "id": "q5NuiyJNjS3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and eval funcs go hereeeee\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2SeqTransformer().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn   = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for (src, slen), (tgt, tlen) in tqdm(train_loader):\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        # input to decoder is all but last token\n",
        "        out = model(src, tgt[:,:-1])\n",
        "        # compute loss against next tokens\n",
        "        loss = loss_fn(out.reshape(-1, VOCAB_SIZE), tgt[:,1:].reshape(-1))\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for (src, slen), (tgt, tlen) in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        out = model(src, tgt[:,:-1])\n",
        "        loss = loss_fn(out.reshape(-1, VOCAB_SIZE), tgt[:,1:].reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def greedy_decode(src, max_len=128):\n",
        "    src = src.to(device)\n",
        "    memory = model.encode(src)\n",
        "    ys = torch.full((src.size(0),1), BOS, device=device, dtype=torch.long)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(ys, memory)\n",
        "        prob = model.generator(out[:,-1,:])\n",
        "        next_word = prob.argmax(dim=-1, keepdim=True)\n",
        "        ys = torch.cat([ys, next_word], dim=0)\n",
        "        if (next_word==EOS).all(): break\n",
        "    return ys.cpu().tolist()\n"
      ],
      "metadata": {
        "id": "PEpqajPKj-Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch} | train loss {train_loss:.4f} | val loss {val_loss:.4f} | {time.time()-t0:.1f}s\")\n",
        "    torch.save(model.state_dict(), SAVE_DIR/f\"epoch{epoch}.pt\")\n"
      ],
      "metadata": {
        "id": "ONVjaxypkOvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  testing here\n",
        "\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "x = sp.Symbol('x')\n",
        "\n",
        "for (src, slen), (tgt, tlen) in tqdm(test_loader):\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps = greedy_decode(src)       # list of B lists of token IDs - all hypostheses\n",
        "    truths = tgt.tolist()           # list of B lists - truths\n",
        "\n",
        "    for hyp_ids, true_ids in zip(hyps, truths):\n",
        "        try:\n",
        "            # strip special tokens\n",
        "            hyp_tok  = [idx2word[i] for i in hyp_ids  if i not in (PAD, BOS, EOS)]\n",
        "            true_tok = [idx2word[i] for i in true_ids if i not in (PAD, BOS, EOS)]\n",
        "\n",
        "            # convert to Sympy and check\n",
        "            hyp_expr  = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "            if verify_solution(sp.diff(hyp_expr, x), hyp_expr, x):\n",
        "                n_correct += 1\n",
        "        except Exception:\n",
        "            pass\n",
        "        total += 1\n",
        "\n",
        "acc = 100 * n_correct / len(test_ds)\n",
        "print(f\"Greedy semantic accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "DfjkSAYrkU3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write beam search here\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "\n",
        "BeamHyp = namedtuple(\"BeamHyp\", [\"score\", \"tokens\"])\n",
        "\n",
        "def beam_search(src_batch, beam_size=5, length_penalty=1.0, max_len=128):\n",
        "    \"\"\"\n",
        "    src_batch: LongTensor (B, S) - batch first as in the main model too\n",
        "    returns: list of B best token ID lists\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    B, S = src_batch.shape\n",
        "    src_batch = src_batch.to(device)\n",
        "    memory = model.encode(src_batch)\n",
        "\n",
        "    # initialize beams per example\n",
        "    beams = [[BeamHyp(0.0, [BOS])] for _ in range(B)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_beams = [[] for _ in range(B)]\n",
        "        for b in range(B):\n",
        "            for hyp in beams[b]:\n",
        "                tokens = hyp.tokens\n",
        "                # prepare decoder input: (1, t)\n",
        "                tgt_input = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "                dec = model.decode(tgt_input, memory[b:b+1])     # (1, t, D)\n",
        "                # project last step to vocab & log‐softmax\n",
        "                logits = model.generator(dec[:, -1, :])          # (1, V)\n",
        "                logp   = F.log_softmax(logits, dim=-1).squeeze(0) # (V,)\n",
        "\n",
        "                topv, topi = logp.topk(beam_size)\n",
        "                for score, idx in zip(topv.tolist(), topi.tolist()):\n",
        "                    all_beams[b].append(BeamHyp(hyp.score + score, tokens + [idx]))\n",
        "\n",
        "            # prune back to beam_size\n",
        "            all_beams[b].sort(\n",
        "                key=lambda h: h.score / (len(h.tokens) ** length_penalty),\n",
        "                reverse=True\n",
        "            )\n",
        "            beams[b] = all_beams[b][:beam_size]\n",
        "\n",
        "    # extract best sequence per example\n",
        "    results = []\n",
        "    for b in range(B):\n",
        "        best = max(\n",
        "            beams[b],\n",
        "            key=lambda h: h.score / (len(h.tokens) ** length_penalty)\n",
        "        )\n",
        "        results.append(best.tokens)\n",
        "    return results\n",
        "\n",
        "# now evaluate with beam=10\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "for (src, slen), (tgt, tlen) in tqdm(test_loader):\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps   = beam_search(src, beam_size=10, length_penalty=1.0, max_len=128)\n",
        "    truths = tgt.tolist()\n",
        "\n",
        "    for hyp_ids, true_ids in zip(hyps, truths):\n",
        "        try:\n",
        "            hyp_tok = [idx2word[i] for i in hyp_ids  if i not in (PAD,BOS,EOS)]\n",
        "            hyp_expr = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "            x = sp.Symbol('x')\n",
        "            if verify_solution(sp.diff(hyp_expr, x), hyp_expr, x):\n",
        "                n_correct += 1\n",
        "        except:\n",
        "            pass\n",
        "        total += 1\n",
        "\n",
        "acc = 100 * n_correct / total\n",
        "print(f\"Beam-10 semantic accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "QNgmj8xIod5n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}