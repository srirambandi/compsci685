{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirambandi/compsci685/blob/main/train_and_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDNXFDkJfj2d",
        "outputId": "28a96839-282b-4a24-f0e7-88983ef4f80e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'compsci685'...\n",
            "remote: Enumerating objects: 153, done.\u001b[K\n",
            "remote: Counting objects: 100% (153/153), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 153 (delta 77), reused 103 (delta 35), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (153/153), 25.90 MiB | 3.05 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n",
            "/content/compsci685\n"
          ]
        }
      ],
      "source": [
        "# Colab Cell – clone your repo at the top of the notebook\n",
        "!git clone https://github.com/srirambandi/compsci685.git\n",
        "%cd compsci685"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Zcfni4zAYe",
        "outputId": "b3e578f3-61ac-4178-c4db-24a35f886ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers datasets sympy tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfE1qm7Hf3dg"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"gen_dataset\")\n",
        "sys.path.insert(0, \"gen_dataset/src\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wVrxNl94c69"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from utils import verify_solution\n",
        "\n",
        "x = sp.Symbol('x', real=True, nonzero=True)\n",
        "c = sp.Symbol('c', real=True)\n",
        "f = sp.Function('f', real=True, nonzero=True)\n",
        "\n",
        "\n",
        "def prefix_to_sympy(prefix, arity_map):\n",
        "    def helper(tokens):\n",
        "        # pop the first token\n",
        "        token = tokens.pop(0)\n",
        "\n",
        "        if token == 'x':\n",
        "            return x\n",
        "        elif token == 'c':\n",
        "            return c\n",
        "        elif token == 'y':\n",
        "            return f(x)\n",
        "        elif token == \"y'\":\n",
        "            return Derivative(f(x), x)\n",
        "        \n",
        "\n",
        "        if token.isdigit():\n",
        "            return sp.Integer(int(token))\n",
        "\n",
        "        # mathematical constants\n",
        "        if token == 'E':\n",
        "            return sp.E\n",
        "        if token == 'pi':\n",
        "            return sp.pi\n",
        "\n",
        "        elif token in arity_map:\n",
        "            arity = arity_map[token]\n",
        "            _args = []\n",
        "            for _ in range(arity):\n",
        "                arg = helper(tokens)\n",
        "                _args.append(arg)\n",
        "\n",
        "            # basic operators\n",
        "            if token == 'add':\n",
        "                return sp.Add(*_args)\n",
        "            elif token == 'sub':\n",
        "                return sp.Add(_args[0], sp.Mul(-1, _args[1]))\n",
        "            elif token == 'mul':\n",
        "                return sp.Mul(*_args)\n",
        "            elif token == 'div':\n",
        "                return sp.Mul(_args[0], sp.Pow(_args[1], -1))\n",
        "            elif token == 'pow':\n",
        "                return sp.Pow(*_args)\n",
        "            elif token == 'sqrt':\n",
        "                return sp.sqrt(_args[0])\n",
        "            elif token == 'exp':\n",
        "                return sp.exp(_args[0])\n",
        "            elif token == 'log':\n",
        "                return sp.log(_args[0])\n",
        "            elif token == 'abs':\n",
        "                return sp.Abs(_args[0])\n",
        "            elif token == 'sign':\n",
        "                return sp.sign(_args[0])\n",
        "            elif token in ['INT-', 'INT+']:\n",
        "                return sp.Integer(_args[0] * -1 if token == 'INT-' else _args[0])\n",
        "\n",
        "            # trig operators\n",
        "            elif token == 'sin':\n",
        "                return sp.sin(_args[0])\n",
        "            elif token == 'cos':\n",
        "                return sp.cos(_args[0])\n",
        "            elif token == 'tan':\n",
        "                return sp.tan(_args[0])\n",
        "\n",
        "            # inverse trig operators\n",
        "            elif token == 'asin':\n",
        "                return sp.asin(_args[0])\n",
        "            elif token == 'acos':\n",
        "                return sp.acos(_args[0])\n",
        "            elif token == 'atan':\n",
        "                return sp.atan(_args[0])\n",
        "\n",
        "            # hyperbolic operators\n",
        "            elif token == 'sinh':\n",
        "                return sp.sinh(_args[0])\n",
        "            elif token == 'cosh':\n",
        "                return sp.cosh(_args[0])\n",
        "            elif token == 'tanh':\n",
        "                return sp.tanh(_args[0])\n",
        "\n",
        "            # inverse hyperbolic operators\n",
        "            elif token == 'asinh':\n",
        "                return sp.asinh(_args[0])\n",
        "            elif token == 'acosh':\n",
        "                return sp.acosh(_args[0])\n",
        "            elif token == 'atanh':\n",
        "                return sp.atanh(_args[0])\n",
        "            else:\n",
        "                print(f\"Unknown operator: {token}\")\n",
        "        else:\n",
        "            print(f\"Unknown token: {token}\")\n",
        "\n",
        "    tokens = list(prefix)\n",
        "    return helper(tokens)\n",
        "\n",
        "\n",
        "OPERATORS = {\n",
        "    # basic - binary\n",
        "    'add': 2, 'sub': 2, 'mul': 2, 'div': 2, 'pow': 2,\n",
        "    # basic - unary\n",
        "    'sqrt': 1, 'exp': 1, 'log': 1, 'abs': 1, 'sign': 1,\n",
        "    # trig - unary\n",
        "    'sin': 1, 'cos': 1, 'tan': 1,\n",
        "    # inverse trig - unary\n",
        "    'asin': 1, 'acos': 1, 'atan': 1,\n",
        "    # hyperbolic - unary\n",
        "    'sinh': 1, 'cosh': 1, 'tanh': 1,\n",
        "    # inverse hyperbolic - unary\n",
        "    'asinh': 1, 'acosh': 1, 'atanh': 1,\n",
        "    # INT - unary\n",
        "    'INT+': 1, 'INT-': 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_C7ketupi8Q",
        "outputId": "190b6c45-f407-45bc-bee0-adfb0c922cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "SAVE_DIR = \"/content/drive/MyDrive/compsci685/checkpoints_baseline_paper_ode1_dataset\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW5GaQqd48vx",
        "outputId": "243b3534-c11d-4f7d-cef3-590d6705c815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting Data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "683442it [00:00, 1489316.55it/s]\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"gen_dataset/data/dataset2.txt\"\n",
        "# data is stored as input_equation\\toutput_equation\n",
        "\n",
        "TEST_SIZE = 1_024 # number of samples to evaluate model equation correctness on\n",
        "VALID_SIZE = 2_048 # number of samples to get validation loss from\n",
        "# hardcode these numbers, as 10% of new >600K sample dataset is too much\n",
        "# evaluation is slow, so only take 1024\n",
        "# validation is a bit faster, so take 2048\n",
        "# use those numbers as they are powers of 2 closest to 1k and 2k\n",
        "# so batch size will evenly divide them during validation\n",
        "\n",
        "os.makedirs(\"splits\", exist_ok=True)\n",
        "from tqdm import tqdm\n",
        "print(\"Splitting Data\")\n",
        "with open(DATA_PATH, 'r') as reader:\n",
        "  i = 0\n",
        "  with open(\"splits/test.txt\", 'w') as test_writer, open(\"splits/valid.txt\", 'w') as valid_writer, open(\"splits/train.txt\", 'w') as train_writer:\n",
        "    for line in tqdm(reader):\n",
        "      if i < TEST_SIZE:\n",
        "        test_writer.write(line + \"\\n\")\n",
        "      elif i < (TEST_SIZE + VALID_SIZE):\n",
        "        valid_writer.write(line + \"\\n\")\n",
        "      else:\n",
        "        train_writer.write(line + \"\\n\")\n",
        "\n",
        "      i += 1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Aba40sdOoK",
        "outputId": "9bfc31b9-09ee-4af8-f052-a6f083f5b369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1360740 splits/train.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l splits/train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywlJLwoyi2xw",
        "outputId": "ee3cff8a-d825-46ce-a628-7488860f0605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'0': 3, '1': 4, '2': 5, '3': 6, '4': 7, '5': 8, '6': 9, '7': 10, '8': 11, '9': 12, 'E': 13, 'INT+': 14, 'INT-': 15, 'abs': 16, 'acos': 17, 'acosh': 18, 'add': 19, 'asin': 20, 'asinh': 21, 'atan': 22, 'atanh': 23, 'c': 24, 'cos': 25, 'cosh': 26, 'div': 27, 'exp': 28, 'log': 29, 'mul': 30, 'pi': 31, 'pow': 32, 'sign': 33, 'sin': 34, 'sinh': 35, 'sqrt': 36, 'tan': 37, 'tanh': 38, 'x': 39, 'y': 40, \"y'\": 41, '<pad>': 0, '<bos>': 1, '<eos>': 2}\n"
          ]
        }
      ],
      "source": [
        "# special tokens\n",
        "SPECIAL = {\"<pad>\":0,\"<bos>\":1,\"<eos>\":2}\n",
        "\n",
        "# collect tokens from original file:\n",
        "tokens = set()\n",
        "with open(DATA_PATH, 'r') as reader:\n",
        "  for line in reader:\n",
        "    tokens.update(line.split())\n",
        "word2idx = {w:i+len(SPECIAL) for i,w in enumerate(sorted(tokens))}\n",
        "word2idx.update(SPECIAL)\n",
        "print(word2idx)\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "PAD, BOS, EOS = word2idx[\"<pad>\"], word2idx[\"<bos>\"], word2idx[\"<eos>\"]\n",
        "VOCAB_SIZE = len(word2idx)\n",
        "\n",
        "class ODEDataset(Dataset):\n",
        "    def __init__(self, data_file, max_len=18):\n",
        "        self.src = []\n",
        "        self.tgt = []\n",
        "        with open(data_file, 'r') as reader:\n",
        "          for line in reader:\n",
        "            if \"\\t\" in line: # last line doesn't have any data\n",
        "              src_item, tgt_item = line.split(\"\\t\")\n",
        "              self.src.append(src_item.split())\n",
        "              self.tgt.append(tgt_item.split())\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "    def __getitem__(self, i):\n",
        "        try:\n",
        "          src = [BOS] + [word2idx[t] for t in self.src[i]] + [EOS]\n",
        "          tgt = [BOS] + [word2idx[t] for t in self.tgt[i]] + [EOS]\n",
        "        except:\n",
        "          src = [BOS, EOS]\n",
        "          tgt = [BOS, EOS]\n",
        "\n",
        "        def pad(x):\n",
        "            x = x[:self.max_len]\n",
        "            return x + [PAD]*(self.max_len-len(x))\n",
        "        return torch.tensor(pad(src)), len(src), torch.tensor(pad(tgt)), len(tgt)\n",
        "\n",
        "def collate(batch):\n",
        "    srcs, slens, tgts, tlens = zip(*batch)\n",
        "    return (torch.stack(srcs), torch.tensor(slens)), (torch.stack(tgts), torch.tensor(tlens))\n",
        "\n",
        "BATCH=256\n",
        "train_loader = DataLoader(ODEDataset(\"splits/train.txt\"), batch_size=BATCH, shuffle=True, collate_fn=collate, drop_last=True) # drop last when training because shuffled + multi epochs\n",
        "val_loader   = DataLoader(ODEDataset(\"splits/valid.txt\"), batch_size=BATCH, shuffle=False, collate_fn=collate)\n",
        "test_loader  = DataLoader(ODEDataset(\"splits/test.txt\"), batch_size=32, shuffle=False, collate_fn=collate) # smaller batch size to allow testing smaller subsets during development\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5NuiyJNjS3o"
      },
      "outputs": [],
      "source": [
        "# TODO: check if we should update this!! - a smaller model than the one in original Deep Learning for Symbolic Mathematcs paper\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model=256,\n",
        "                 nhead=4,\n",
        "                 num_encoder_layers=4,\n",
        "                 num_decoder_layers=4,\n",
        "                 dim_feedforward=512,\n",
        "                 dropout=0.1,\n",
        "                 max_len=64):\n",
        "        super().__init__()\n",
        "        # positional embeddings: (max_len, d_model)\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(max_len, d_model))\n",
        "        self.embedding = nn.Embedding(VOCAB_SIZE, d_model, padding_idx=PAD)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model, nhead,\n",
        "            num_encoder_layers, num_decoder_layers,\n",
        "            dim_feedforward, dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.generator = nn.Linear(d_model, VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # src: (S, B), tgt: (T, B)\n",
        "        B, S = src.shape\n",
        "        B2, T = tgt.shape\n",
        "        assert B == B2\n",
        "\n",
        "        # (B,S,d_model) + (1,S,d_model) => broadcast to (B,S,d)\n",
        "        src_emb = self.embedding(src) + self.pos_enc[:S].unsqueeze(0)\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_enc[:T].unsqueeze(0)\n",
        "\n",
        "\n",
        "        out = self.transformer(\n",
        "            src_emb, tgt_emb,\n",
        "            src_key_padding_mask=src == PAD,\n",
        "            tgt_key_padding_mask=tgt == PAD,\n",
        "            memory_key_padding_mask=src == PAD,\n",
        "            tgt_mask=self.transformer.generate_square_subsequent_mask(T).to(src.device)\n",
        "        )\n",
        "        return self.generator(out)  # (B, T, V) expected here\n",
        "\n",
        "    def encode(self, src):\n",
        "        B, S = src.shape\n",
        "        src_emb = self.embedding(src) + self.pos_enc[:S].unsqueeze(0)\n",
        "        return self.transformer.encoder(\n",
        "            src_emb,\n",
        "            src_key_padding_mask=src == PAD\n",
        "        )\n",
        "\n",
        "    def decode(self, tgt, memory):\n",
        "        B, T = tgt.shape\n",
        "        tgt_emb = self.embedding(tgt) + self.pos_enc[:T].unsqueeze(0)\n",
        "        return self.transformer.decoder(\n",
        "            tgt_emb,\n",
        "            memory,\n",
        "            tgt_mask=self.transformer.generate_square_subsequent_mask(T).to(tgt.device),\n",
        "            tgt_key_padding_mask=tgt == PAD\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEpqajPKj-Np"
      },
      "outputs": [],
      "source": [
        "# training and eval funcs go hereeeee\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Seq2SeqTransformer().to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "loss_fn   = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    with tqdm(train_loader) as pbar:\n",
        "        for (src, slen), (tgt, tlen) in pbar:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            # input to decoder is all but last token\n",
        "            out = model(src, tgt[:,:-1])\n",
        "            # compute loss against next tokens\n",
        "            loss = loss_fn(out.reshape(-1, VOCAB_SIZE), tgt[:,1:].reshape(-1))\n",
        "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for (src, slen), (tgt, tlen) in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        out = model(src, tgt[:,:-1])\n",
        "        loss = loss_fn(out.reshape(-1, VOCAB_SIZE), tgt[:,1:].reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def greedy_decode(src, max_len=18):\n",
        "    src = src.to(device)\n",
        "    memory = model.encode(src)\n",
        "    ys = torch.full((src.size(0),1), BOS, device=device, dtype=torch.long)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(ys, memory)\n",
        "        prob = model.generator(out[:,-1,:])\n",
        "        next_word = prob.argmax(dim=-1, keepdim=True)\n",
        "        ys = torch.cat([ys, next_word], dim=1)\n",
        "        if (next_word==EOS).all(): break\n",
        "    return ys.cpu().tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znMLQw_nq3tR"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def loss_tester(loader):\n",
        "    total_loss = 0\n",
        "    for (src, slen), (tgt, tlen) in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        out = torch.nn.functional.one_hot(tgt[:, 1:], VOCAB_SIZE)\n",
        "        loss = loss_fn(out.reshape(-1, VOCAB_SIZE).float() * 1e10, tgt[:,1:].reshape(-1))\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONVjaxypkOvF",
        "outputId": "cc4685a9-5049-492b-b859-c0980b4025c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/2657 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "100%|██████████| 2657/2657 [01:48<00:00, 24.42it/s, loss=0.985]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | train loss 1.2860 | val loss 0.7722 | 109.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.51it/s, loss=0.873]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | train loss 0.9222 | val loss 0.6982 | 108.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:47<00:00, 24.61it/s, loss=0.782]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | train loss 0.8300 | val loss 0.6331 | 108.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:50<00:00, 24.02it/s, loss=0.777]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | train loss 0.7762 | val loss 0.5925 | 110.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.54it/s, loss=0.702]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | train loss 0.7360 | val loss 0.5871 | 108.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.54it/s, loss=0.673]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | train loss 0.7001 | val loss 0.5505 | 108.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:47<00:00, 24.62it/s, loss=0.636]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | train loss 0.6684 | val loss 0.5339 | 108.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:50<00:00, 24.00it/s, loss=0.645]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | train loss 0.6412 | val loss 0.5309 | 110.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.47it/s, loss=0.599]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | train loss 0.6174 | val loss 0.4996 | 108.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.49it/s, loss=0.579]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | train loss 0.5964 | val loss 0.4795 | 108.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.44it/s, loss=0.534]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | train loss 0.5766 | val loss 0.4697 | 108.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:50<00:00, 23.95it/s, loss=0.523]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | train loss 0.5590 | val loss 0.4592 | 111.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.49it/s, loss=0.555]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | train loss 0.5425 | val loss 0.4341 | 108.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.56it/s, loss=0.521]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | train loss 0.5272 | val loss 0.4496 | 108.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.93it/s, loss=0.512]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | train loss 0.5136 | val loss 0.4290 | 111.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.54it/s, loss=0.515]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | train loss 0.5011 | val loss 0.4288 | 108.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.52it/s, loss=0.448]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | train loss 0.4897 | val loss 0.4155 | 108.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.53it/s, loss=0.472]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | train loss 0.4796 | val loss 0.4034 | 108.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:50<00:00, 24.01it/s, loss=0.462]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | train loss 0.4702 | val loss 0.3933 | 110.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.42it/s, loss=0.467]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 | train loss 0.4613 | val loss 0.3928 | 109.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.40it/s, loss=0.454]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 | train loss 0.4534 | val loss 0.4068 | 109.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.42it/s, loss=0.453]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 | train loss 0.4457 | val loss 0.3865 | 109.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.92it/s, loss=0.442]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 | train loss 0.4392 | val loss 0.3797 | 111.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:49<00:00, 24.36it/s, loss=0.409]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 | train loss 0.4330 | val loss 0.3819 | 109.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.50it/s, loss=0.411]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 | train loss 0.4269 | val loss 0.3876 | 108.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.85it/s, loss=0.376]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 | train loss 0.4214 | val loss 0.3834 | 111.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:49<00:00, 24.37it/s, loss=0.421]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 | train loss 0.4161 | val loss 0.3682 | 109.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.43it/s, loss=0.425]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 | train loss 0.4114 | val loss 0.3991 | 108.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.52it/s, loss=0.393]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 | train loss 0.4068 | val loss 0.3764 | 108.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:50<00:00, 24.01it/s, loss=0.393]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 | train loss 0.4024 | val loss 0.3798 | 110.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.41it/s, loss=0.402]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31 | train loss 0.3985 | val loss 0.3705 | 109.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.51it/s, loss=0.405]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32 | train loss 0.3946 | val loss 0.3769 | 108.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.49it/s, loss=0.351]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33 | train loss 0.3909 | val loss 0.3713 | 108.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:50<00:00, 23.98it/s, loss=0.397]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34 | train loss 0.3873 | val loss 0.3660 | 110.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.47it/s, loss=0.404]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35 | train loss 0.3844 | val loss 0.3616 | 108.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.53it/s, loss=0.389]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36 | train loss 0.3812 | val loss 0.3558 | 108.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.74it/s, loss=0.37]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37 | train loss 0.3781 | val loss 0.3775 | 112.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.55it/s, loss=0.396]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38 | train loss 0.3755 | val loss 0.3438 | 108.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.55it/s, loss=0.346]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39 | train loss 0.3726 | val loss 0.3645 | 108.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.53it/s, loss=0.386]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40 | train loss 0.3701 | val loss 0.3645 | 108.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.90it/s, loss=0.363]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41 | train loss 0.3677 | val loss 0.3609 | 111.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:48<00:00, 24.39it/s, loss=0.349]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42 | train loss 0.3651 | val loss 0.3679 | 109.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:49<00:00, 24.28it/s, loss=0.335]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43 | train loss 0.3630 | val loss 0.3555 | 109.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.91it/s, loss=0.382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44 | train loss 0.3607 | val loss 0.3554 | 111.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:53<00:00, 23.48it/s, loss=0.369]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45 | train loss 0.3586 | val loss 0.3666 | 113.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:49<00:00, 24.20it/s, loss=0.403]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46 | train loss 0.3564 | val loss 0.3772 | 109.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:49<00:00, 24.17it/s, loss=0.319]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47 | train loss 0.3547 | val loss 0.3647 | 110.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:54<00:00, 23.16it/s, loss=0.382]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48 | train loss 0.3528 | val loss 0.3569 | 114.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:51<00:00, 23.87it/s, loss=0.338]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 | train loss 0.3510 | val loss 0.3664 | 111.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2657/2657 [01:53<00:00, 23.50it/s, loss=0.352]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 | train loss 0.3493 | val loss 0.3598 | 113.2s\n"
          ]
        }
      ],
      "source": [
        "# training loop\n",
        "EPOCHS = 50\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # make faster\n",
        "\n",
        "\n",
        "best_model_state = None\n",
        "best_val_loss = float(\"inf\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_epoch()\n",
        "    val_loss   = evaluate(val_loader)\n",
        "    print(f\"Epoch {epoch} | train loss {train_loss:.4f} | val loss {val_loss:.4f} | {time.time()-t0:.1f}s\")\n",
        "    torch.save(model.state_dict(), SAVE_DIR + f\"/epoch{epoch}.pt\")\n",
        "    if best_val_loss > val_loss:\n",
        "      best_model_state = model.state_dict()\n",
        "      best_val_loss = val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEAx2o5gqhZz",
        "outputId": "db361716-3ed4-4b84-adde-b04b79713f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss 0.6284813797101378\n",
            "Best possible loss 0.0\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(test_loader)\n",
        "print(\"Test loss\", test_loss)\n",
        "loss_test = loss_tester(test_loader)\n",
        "print(\"Best possible loss\", loss_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvmUV6M54nRE"
      },
      "outputs": [],
      "source": [
        "# get average test output length\n",
        "avg_tgt_len = 0\n",
        "num = 0\n",
        "for (src, src_lens), (tgt, tgt_lens) in test_loader:\n",
        "  for tgt_len in tgt_lens:\n",
        "    avg_tgt_len += tgt_len.item()\n",
        "    num += 1\n",
        "\n",
        "avg_tgt_len /= num\n",
        "print(\"Average output length\", avg_tgt_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfjkSAYrkU3I",
        "outputId": "ddd90224-5f19-4bbe-d4ff-ad12376ca57a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [11:01<00:00, 20.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy semantic accuracy: 0.10%\n",
            "gt ['mul', 'c', 'pow', 'add', 'INT+', '4', 'mul', 'INT+', '2', 'x', 'INT-', '1']\n",
            "model ['mul', 'c', 'pow', 'add', 'INT+', '1', '2', 'mul', 'INT+', '4', 'x', 'INT-', '1']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#  testing here\n",
        "\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "x = sp.Symbol('x')\n",
        "rng = np.random.default_rng()\n",
        "\n",
        "for (src, slen), (tgt, tlen) in tqdm(test_loader):\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps = greedy_decode(src)       # list of B lists of token IDs - all hypostheses\n",
        "    truths = tgt.tolist()           # list of B lists - truths\n",
        "\n",
        "    for hyp_ids, true_ids in zip(hyps, truths):\n",
        "        # find first EOS and remove everything after it\n",
        "        try:\n",
        "          first_eos = hyp_ids.index(EOS)\n",
        "        except:\n",
        "          first_eos = len(hyp_ids) # if no EOS, don't strip\n",
        "\n",
        "        # strip special tokens\n",
        "        hyp_tok  = [idx2word[i] for i in hyp_ids[:first_eos]  if i not in (PAD, BOS, EOS)]\n",
        "        true_tok = [idx2word[i] for i in true_ids if i not in (PAD, BOS, EOS)]\n",
        "\n",
        "        # convert to Sympy and check\n",
        "        try:\n",
        "          hyp_expr = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "        except:\n",
        "          total += 1 # if model output isn't real equation, skip\n",
        "          continue\n",
        "        true_expr = prefix_to_sympy(true_tok, OPERATORS)\n",
        "        if verify_solution(hyp_expr, true_expr, rng):\n",
        "            n_correct += 1\n",
        "\n",
        "\n",
        "        total += 1\n",
        "\n",
        "acc = 100 * n_correct / total\n",
        "print(f\"Greedy semantic accuracy: {acc:.2f}%\")\n",
        "print(\"gt\", true_tok)\n",
        "print(\"model\", hyp_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrLnjnE7dOG_"
      },
      "outputs": [],
      "source": [
        "# write beam search here\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple\n",
        "\n",
        "BeamHyp = namedtuple(\"BeamHyp\", [\"score\", \"tokens\"])\n",
        "\n",
        "def beam_search(src_batch, beam_size=5, length_penalty=1.0, max_len=128):\n",
        "    \"\"\"\n",
        "    src_batch: LongTensor (B, S) - batch first as in the main model too\n",
        "    returns: list of B best token ID lists\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    B, S = src_batch.shape\n",
        "    src_batch = src_batch.to(device)\n",
        "    memory = model.encode(src_batch)\n",
        "\n",
        "    # initialize beams per example\n",
        "    beams = [[BeamHyp(0.0, [BOS])] for _ in range(B)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_beams = [[] for _ in range(B)]\n",
        "        for b in range(B):\n",
        "            for hyp in beams[b]:\n",
        "                tokens = hyp.tokens\n",
        "                # prepare decoder input: (1, t)\n",
        "                tgt_input = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "                dec = model.decode(tgt_input, memory[b:b+1])     # (1, t, D)\n",
        "                # project last step to vocab & log‐softmax\n",
        "                logits = model.generator(dec[:, -1, :])          # (1, V)\n",
        "                logp   = F.log_softmax(logits, dim=-1).squeeze(0) # (V,)\n",
        "\n",
        "                topv, topi = logp.topk(beam_size)\n",
        "                for score, idx in zip(topv.tolist(), topi.tolist()):\n",
        "                    all_beams[b].append(BeamHyp(hyp.score + score, tokens + [idx]))\n",
        "\n",
        "            # prune back to beam_size\n",
        "            all_beams[b].sort(\n",
        "                key=lambda h: h.score / (len(h.tokens) ** length_penalty),\n",
        "                reverse=True\n",
        "            )\n",
        "            beams[b] = all_beams[b][:beam_size]\n",
        "\n",
        "    # extract all beams, and count as correct if at least one beam from hypothesis is correct\n",
        "    results = []\n",
        "    for b in range(B):\n",
        "        b_beams = beams[b]\n",
        "        out_beams = [a.tokens for a in b_beams]\n",
        "        results.append(out_beams)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNgmj8xIod5n",
        "outputId": "3522729f-7342-4ac3-99df-3216504e9ddf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [2:24:31<00:00, 270.99s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam-10 semantic accuracy: 0.78%\n",
            "model ['mul', 'c', 'pow', 'add', 'INT+', '1', '0', 'mul', 'INT+', '5', 'x', 'INT-', '1']\n",
            "gt ['mul', 'c', 'pow', 'add', 'INT+', '4', 'mul', 'INT+', '2', 'x', 'INT-', '1']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# now evaluate with beam=10\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "for (src, slen), (tgt, tlen) in tqdm(test_loader):\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps   = beam_search(src, beam_size=10, length_penalty=1.0, max_len=18)\n",
        "    truths = tgt.tolist()\n",
        "\n",
        "    for beam, true_ids in zip(hyps, truths):\n",
        "        for hyp_ids in beam: # if one item from beams is correct, then count sample as correct\n",
        "          # find first EOS and remove everything after it\n",
        "          try:\n",
        "            first_eos = hyp_ids.index(EOS)\n",
        "          except:\n",
        "            first_eos = len(hyp_ids) # if no EOS, don't strip\n",
        "\n",
        "          # strip special tokens\n",
        "          hyp_tok  = [idx2word[i] for i in hyp_ids[:first_eos]  if i not in (PAD, BOS, EOS)]\n",
        "          true_tok = [idx2word[i] for i in true_ids if i not in (PAD, BOS, EOS)]\n",
        "\n",
        "          # convert to Sympy and check\n",
        "          try:\n",
        "            hyp_expr = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "          except:\n",
        "            continue # if model output isn't real equation, skip this beam\n",
        "          true_expr = prefix_to_sympy(true_tok, OPERATORS)\n",
        "          if verify_solution(hyp_expr, true_expr, rng):\n",
        "              n_correct += 1\n",
        "              break # found correct answer, can stop looking for this set of beams / input\n",
        "        total += 1\n",
        "\n",
        "\n",
        "acc = 100 * n_correct / total\n",
        "print(f\"Beam-10 semantic accuracy: {acc:.2f}%\")\n",
        "print(\"model\", hyp_tok)\n",
        "print(\"gt\", true_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "a648f07e7ca940e299cce73bbf51b106",
            "704167cba5ca4a43ac9e0ed2b1c35b6d",
            "6bd3a0b04bb64688a67fd60da64f5495",
            "9bd2a00ec8544fcb861d9555c960bbe7",
            "0c6effd7973b4c36a4a3c49087770f85",
            "3b0e90fcfb5b4f15a0fcebc63a655d5a",
            "274ca2efd18d4f39a0f5c98908ec10b3",
            "6680b83bc60742d487d1049716bca401",
            "265472d94c9a4e349406ad6ce3fe5576",
            "7de60789d1d2470ea76da095e13bf307",
            "198eaf7d8b1045c58b7ca68f969dce31"
          ]
        },
        "id": "S3pUeuzb4wbF",
        "outputId": "fd37bf4b-7620-4841-9a5b-006a0fb67a9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a648f07e7ca940e299cce73bbf51b106",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/106 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Beam-3 semantic accuracy: 0.00%\n",
            "model ['add', 'add', 'add', 'add', 'add', 'add', 'c', 'x', 'add', 'add', 'c', 'x', 'add', 'add', '3', 'x', 'pow', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'c', 'x', '3', 'x', 'add', 'add', 'add', 'c', 'x', 'add', 'add', 'add', 'add', 'c', 'x', 'add', 'add', 'add', 'add', 'add', 'c', 'x', '3', 'x', 'add', 'add', 'add', 'add', 'c', 'x', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'add', 'c', 'x', '3', 'c', 'x', '3', 'c', 'x', '3', 'c', 'x', 'add', 'add', 'add', 'add', '3', 'c', 'x', 'add', 'add', 'add', 'add', '3', 'c', 'x', 'add', 'add', 'add', 'add', '3', 'c', 'x', 'add', 'add', 'add', 'add', 'add', '3', 'c', 'x', 'add', 'add', 'add', 'add', 'add', '3', 'c', 'x', 'add', 'add', 'add']\n",
            "gt ['add', 'add', 'c', 'mul', '-3', 'x', 'mul', '-3', 'cos', 'x']\n"
          ]
        }
      ],
      "source": [
        "# now evaluate with beam=3\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "total = 0\n",
        "for (src, slen), (tgt, tlen) in tqdm(test_loader):\n",
        "    src, tgt = src.to(device), tgt.to(device)\n",
        "    hyps   = beam_search(src, beam_size=3, length_penalty=1.0, max_len=16)\n",
        "    truths = tgt.tolist()\n",
        "\n",
        "    for hyp_ids, true_ids in zip(hyps, truths):\n",
        "        try:\n",
        "            hyp_tok = [idx2word[i] for i in hyp_ids  if i not in (PAD,BOS,EOS)]\n",
        "            true_tok = [idx2word[i] for i in true_ids  if i not in (PAD,BOS,EOS)]\n",
        "            hyp_expr = prefix_to_sympy(hyp_tok, OPERATORS)\n",
        "            x = sp.Symbol('x')\n",
        "            if verify_solution(sp.diff(hyp_expr, x), hyp_expr, x):\n",
        "                n_correct += 1\n",
        "        except:\n",
        "            pass\n",
        "        total += 1\n",
        "    if total > 150:\n",
        "      break\n",
        "\n",
        "acc = 100 * n_correct / total\n",
        "print(f\"Beam-3 semantic accuracy: {acc:.2f}%\")\n",
        "print(\"model\", hyp_tok)\n",
        "print(\"gt\", true_tok)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KThmGaeF7Oo",
        "outputId": "6fab43c7-2fe0-465c-e932-9c5d100055ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load model\n",
        "model.load_state_dict(torch.load(SAVE_DIR + \"/epoch100.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQje-piKGH35",
        "outputId": "c6d0e2cf-5239-4cff-e804-094b3209b437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 209, 2]\n"
          ]
        }
      ],
      "source": [
        "# test sequence\n",
        "test_src = [BOS] + [word2idx[t] for t in [\"y'\"]] + [EOS]\n",
        "print(test_src)\n",
        "test_src = torch.LongTensor(test_src).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwH3gzW0Ggnm",
        "outputId": "32955b14-c854-4514-b50a-d65c0e47faa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model ['pow', 'add', 'c', 'x', '-1']\n",
            "model input [\"y'\"]\n",
            "model ['pow', 'mul', 'c', 'x', '-1']\n",
            "model input [\"y'\"]\n",
            "model ['div', 'c', 'x']\n",
            "model input [\"y'\"]\n",
            "model ['exp', 'add', 'c', 'x']\n",
            "model input [\"y'\"]\n",
            "model ['pow', 'sub', 'c', 'x', '-1']\n",
            "model input [\"y'\"]\n",
            "model ['pow', 'add', 'c', 'x']\n",
            "model input [\"y'\"]\n",
            "model ['pow', 'add', 'c', 'x']\n",
            "model input [\"y'\"]\n",
            "model ['div', '4', 'add', 'c', 'x']\n",
            "model input [\"y'\"]\n",
            "model ['div', '4', 'add', 'c', 'x']\n",
            "model input [\"y'\"]\n",
            "model ['div', '-1', 'add', 'c', 'x']\n",
            "model input [\"y'\"]\n"
          ]
        }
      ],
      "source": [
        "# now evaluate with beam=10\n",
        "model.eval()\n",
        "test_src = test_src.to(device)\n",
        "hyps = beam_search(test_src, beam_size=10, length_penalty=1.0, max_len=64)\n",
        "test_src = test_src.tolist()\n",
        "for beam in hyps:\n",
        "    for hyp_ids in beam: # if one item from beams is correct, then count sample as correct\n",
        "      # find first EOS and remove everything after it\n",
        "      try:\n",
        "        first_eos = hyp_ids.index(EOS)\n",
        "      except:\n",
        "        first_eos = len(hyp_ids) # if no EOS, don't strip\n",
        "\n",
        "      # strip special tokens\n",
        "      hyp_tok  = [idx2word[i] for i in hyp_ids[:first_eos]  if i not in (PAD, BOS, EOS)]\n",
        "      input_tok = [idx2word[i] for i in test_src[0] if i not in (PAD, BOS, EOS)]\n",
        "      print(\"model\", hyp_tok)\n",
        "      print(\"model input\", input_tok)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c6effd7973b4c36a4a3c49087770f85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "198eaf7d8b1045c58b7ca68f969dce31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "265472d94c9a4e349406ad6ce3fe5576": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "274ca2efd18d4f39a0f5c98908ec10b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b0e90fcfb5b4f15a0fcebc63a655d5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6680b83bc60742d487d1049716bca401": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd3a0b04bb64688a67fd60da64f5495": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6680b83bc60742d487d1049716bca401",
            "max": 106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_265472d94c9a4e349406ad6ce3fe5576",
            "value": 0
          }
        },
        "704167cba5ca4a43ac9e0ed2b1c35b6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0e90fcfb5b4f15a0fcebc63a655d5a",
            "placeholder": "​",
            "style": "IPY_MODEL_274ca2efd18d4f39a0f5c98908ec10b3",
            "value": "  0%"
          }
        },
        "7de60789d1d2470ea76da095e13bf307": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bd2a00ec8544fcb861d9555c960bbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de60789d1d2470ea76da095e13bf307",
            "placeholder": "​",
            "style": "IPY_MODEL_198eaf7d8b1045c58b7ca68f969dce31",
            "value": " 0/106 [01:20&lt;?, ?it/s]"
          }
        },
        "a648f07e7ca940e299cce73bbf51b106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_704167cba5ca4a43ac9e0ed2b1c35b6d",
              "IPY_MODEL_6bd3a0b04bb64688a67fd60da64f5495",
              "IPY_MODEL_9bd2a00ec8544fcb861d9555c960bbe7"
            ],
            "layout": "IPY_MODEL_0c6effd7973b4c36a4a3c49087770f85"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
